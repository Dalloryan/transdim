{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Augmented Tensor Factorization\n",
    "\n",
    "**Published**: November 12, 2020\n",
    "\n",
    "**Author**: Yixian Chen [[**GitHub homepage**](https://github.com/yxnchen)], Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BATF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Augmented Tensor Factorization (BATF) model on some real-world data sets. In the following, we will discuss:\n",
    "\n",
    "- What the BATF is.\n",
    "\n",
    "- How to implement BATF mainly using Python `numpy` with high efficiency.\n",
    "\n",
    "- How to make imputation on some real-world spatiotemporal datasets.\n",
    "\n",
    "To overcome the problem of missing values within multivariate time series data, this model takes into account low-rank tensor structure by folding data along day dimension. For an in-depth discussion of BATF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Zhaocheng He, Yixian Chen, Yuhuan Lu, Jiawei Wang (2019). <b>Missing traffic data imputation and pattern discovery with a Bayesian augmented tensor factorization model</b>. Transportation Research Part C: Emerging Technologies, 104: 66-77. <a href=\"https://doi.org/10.1016/j.trc.2019.03.003\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary dependencies. We will make use of `numpy` and `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:32.103443Z",
     "start_time": "2020-11-07T08:39:32.069449Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.stats import wishart\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP decomposition\n",
    "\n",
    "#### CP Combination (`cp_combine`)\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "The CP decomposition factorizes a tensor into a sum of outer products of vectors. For example, for a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$, the CP decomposition can be written as\n",
    "\n",
    "$$\\hat{\\mathcal{Y}}=\\sum_{s=1}^{r}\\boldsymbol{u}_{s}\\circ\\boldsymbol{v}_{s}\\circ\\boldsymbol{x}_{s},$$\n",
    "or element-wise,\n",
    "\n",
    "$$\\hat{y}_{ijt}=\\sum_{s=1}^{r}u_{is}v_{js}x_{ts},\\forall (i,j,t),$$\n",
    "where vectors $\\boldsymbol{u}_{s}\\in\\mathbb{R}^{m},\\boldsymbol{v}_{s}\\in\\mathbb{R}^{n},\\boldsymbol{x}_{s}\\in\\mathbb{R}^{f}$ are columns of factor matrices $U\\in\\mathbb{R}^{m\\times r},V\\in\\mathbb{R}^{n\\times r},X\\in\\mathbb{R}^{f\\times r}$, respectively. The symbol $\\circ$ denotes vector outer product.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "Given matrices $U=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{2\\times 2}$, $V=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{3\\times 2}$ and $X=\\left[ \\begin{array}{cc} 1 & 5 \\\\ 2 & 6 \\\\ 3 & 7 \\\\ 4 & 8 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 2}$, then if $\\hat{\\mathcal{Y}}=\\sum_{s=1}^{r}\\boldsymbol{u}_{s}\\circ\\boldsymbol{v}_{s}\\circ\\boldsymbol{x}_{s}$, then, we have\n",
    "\n",
    "$$\\hat{Y}_1=\\hat{\\mathcal{Y}}(:,:,1)=\\left[ \\begin{array}{ccc} 31 & 42 & 65 \\\\ 63 & 86 & 135 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_2=\\hat{\\mathcal{Y}}(:,:,2)=\\left[ \\begin{array}{ccc} 38 & 52 & 82 \\\\ 78 & 108 & 174 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_3=\\hat{\\mathcal{Y}}(:,:,3)=\\left[ \\begin{array}{ccc} 45 & 62 & 99 \\\\ 93 & 130 & 213 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_4=\\hat{\\mathcal{Y}}(:,:,4)=\\left[ \\begin{array}{ccc} 52 & 72 & 116 \\\\ 108 & 152 & 252 \\\\ \\end{array} \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:37.788445Z",
     "start_time": "2020-11-07T08:39:37.772444Z"
    }
   },
   "outputs": [],
   "source": [
    "def cp_combine(var):\n",
    "    return np.einsum('is, js, ts -> ijt', var[0], var[1], var[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:38.494471Z",
     "start_time": "2020-11-07T08:39:38.478443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 31  38  45  52]\n",
      "  [ 42  52  62  72]\n",
      "  [ 65  82  99 116]]\n",
      "\n",
      " [[ 63  78  93 108]\n",
      "  [ 86 108 130 152]\n",
      "  [135 174 213 252]]]\n",
      "\n",
      "tensor size:\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "factor = [np.array([[1, 2], [3, 4]]), np.array([[1, 3], [2, 4], [5, 6]]), \n",
    "          np.array([[1, 5], [2, 6], [3, 7], [4, 8]])]\n",
    "print(cp_combine(factor))\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(cp_combine(factor).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector combination (`vec_combine`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st solution\n",
    "def vec_combine(vector):\n",
    "    tensor = 0\n",
    "    d = len(vector)\n",
    "    for i in range(d):\n",
    "        ax = [len(vector[i]) if j == i else 1 for j in range(d)]\n",
    "        tensor = tensor + vector[i].reshape(ax, order = 'F')\n",
    "    return tensor\n",
    "\n",
    "## 2nd solution\n",
    "def vec_combine(vector):\n",
    "    return (vector[0][:, np.newaxis, np.newaxis] + vector[1][np.newaxis, :, np.newaxis]\n",
    "            + vector[2][np.newaxis, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2]), array([1, 2, 3]), array([1, 2, 3, 4])]\n",
      "[[[3 4 5 6]\n",
      "  [4 5 6 7]\n",
      "  [5 6 7 8]]\n",
      "\n",
      " [[4 5 6 7]\n",
      "  [5 6 7 8]\n",
      "  [6 7 8 9]]]\n",
      "\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "vector = []\n",
    "for i in range(3):\n",
    "    vector.append(np.array([i + 1 for i in range(i + 2)]))\n",
    "print(vector)\n",
    "print(vec_combine(vector))\n",
    "print()\n",
    "print(vector[0][1] + vector[1][1] + vector[2][2])\n",
    "print(vec_combine(vector)[1, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Unfolding (`ten2mat`)\n",
    "\n",
    "Using numpy reshape to perform 3rd rank tensor unfold operation. [[**link**](https://stackoverflow.com/questions/49970141/using-numpy-reshape-to-perform-3rd-rank-tensor-unfold-operation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:40.600444Z",
     "start_time": "2020-11-07T08:39:40.592449Z"
    }
   },
   "outputs": [],
   "source": [
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:41.702444Z",
     "start_time": "2020-11-07T08:39:41.679444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size:\n",
      "(3, 2, 4)\n",
      "original tensor:\n",
      "[[[ 1  2  3  4]\n",
      "  [ 3  4  5  6]]\n",
      "\n",
      " [[ 5  6  7  8]\n",
      "  [ 7  8  9 10]]\n",
      "\n",
      " [[ 9 10 11 12]\n",
      "  [11 12 13 14]]]\n",
      "\n",
      "(1) mode-1 tensor unfolding:\n",
      "[[ 1  3  2  4  3  5  4  6]\n",
      " [ 5  7  6  8  7  9  8 10]\n",
      " [ 9 11 10 12 11 13 12 14]]\n",
      "\n",
      "(2) mode-2 tensor unfolding:\n",
      "[[ 1  5  9  2  6 10  3  7 11  4  8 12]\n",
      " [ 3  7 11  4  8 12  5  9 13  6 10 14]]\n",
      "\n",
      "(3) mode-3 tensor unfolding:\n",
      "[[ 1  5  9  3  7 11]\n",
      " [ 2  6 10  4  8 12]\n",
      " [ 3  7 11  5  9 13]\n",
      " [ 4  8 12  6 10 14]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[[1, 2, 3, 4], [3, 4, 5, 6]], \n",
    "              [[5, 6, 7, 8], [7, 8, 9, 10]], \n",
    "              [[9, 10, 11, 12], [11, 12, 13, 14]]])\n",
    "print('tensor size:')\n",
    "print(X.shape)\n",
    "print('original tensor:')\n",
    "print(X)\n",
    "print()\n",
    "print('(1) mode-1 tensor unfolding:')\n",
    "print(ten2mat(X, 0))\n",
    "print()\n",
    "print('(2) mode-2 tensor unfolding:')\n",
    "print(ten2mat(X, 1))\n",
    "print()\n",
    "print('(3) mode-3 tensor unfolding:')\n",
    "print(ten2mat(X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat2ten(mat, dim, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(dim.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, list(dim[index]), order = 'F'), 0, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Performance Metrics\n",
    "\n",
    "- **RMSE**\n",
    "- **MAPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T08:39:54.534443Z",
     "start_time": "2020-11-07T08:39:54.519443Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample global parameter $\\mu$\n",
    "\n",
    "- Prior distribution:\n",
    "\n",
    "$$\\mu\\sim\\mathcal{N}(\\mu_0,\\tau_0^{-1})$$\n",
    "\n",
    "- Likelihood from:\n",
    "\n",
    "$$y_{i j t} \\sim \\mathcal{N}\\left(\\mu+\\phi_{i}+\\theta_{j}+\\eta_{t}+\\sum_{k=1}^{r} u_{i k} v_{j k} x_{t k}, \\tau^{-1}\\right), \\forall(i, j, t)$$\n",
    "\n",
    "- Posterior distribution:\n",
    "\n",
    "$$\\mu\\mid-\\sim\\mathcal{N}\\left(\\tilde{\\mu},\\tilde{\\tau}^{-1}\\right)$$\n",
    "\n",
    "where $\\tilde{\\tau}=\\tau_0+\\tau|\\Omega|$, $\\tilde{\\mu}=\\tilde{\\tau}^{-1}\\left(\\tau_0\\mu_0+\\tau\\sum_{(i,j,t)\\in\\Omega}\\tilde{y}_{ijt}\\right)$, and $\\tilde{y}_{ijt}=y_{ijt}-(\\phi_{i}+\\theta_{j}+\\eta_{t}+\\sum_{k=1}^{r} u_{i k} v_{j k} x_{t k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_global_mu(mu_sparse, pos_obs, tau_eps, tau0 = 1):\n",
    "    tau_tilde = 1 / (tau_eps * len(pos_obs[0]) + tau0)\n",
    "    mu_tilde = tau_eps * np.sum(mu_sparse) * tau_tilde\n",
    "    return np.random.normal(mu_tilde, np.sqrt(tau_tilde))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample bias vectors $\\boldsymbol{\\phi},\\boldsymbol{\\theta},\\boldsymbol{\\eta}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bias_vector(bias_sparse, factor, bias, ind, dim, k, tau_eps, tau0 = 1):\n",
    "    for k in range(len(dim)):\n",
    "        idx = tuple(filter(lambda x: x != k, range(len(dim))))\n",
    "        temp = vector.copy()\n",
    "        temp[k] = np.zeros((dim[k]))\n",
    "        tau_tilde = 1 / (tau_eps * bias[k] + tau0)\n",
    "        mu_tilde = tau_eps * np.sum(ind * (bias_sparse - vec_combine(temp)), axis = idx) * tau_tilde\n",
    "        vector[k] = np.random.normal(mu_tilde, np.sqrt(tau_tilde))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample factor matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor(tau_sparse, factor, ind, dim, k, tau_eps, beta0 = 1):\n",
    "    dim, rank = factor[k].shape\n",
    "    dim = factor[k].shape[0]\n",
    "    factor_bar = np.mean(factor[k], axis = 0)\n",
    "    temp = dim / (dim + beta0)\n",
    "    var_mu_hyper = temp * factor_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(factor[k], factor_bar) + temp * beta0 * np.outer(factor_bar, factor_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    idx = list(filter(lambda x: x != k, range(len(factor))))\n",
    "    var1 = kr_prod(factor[idx[1]], factor[idx[0]]).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_eps * ind, k).T).reshape([rank, rank, dim]) + var_Lambda_hyper[:, :, np.newaxis]\n",
    "    var4 = var1 @ ten2mat(tau_sparse, k).T + (var_Lambda_hyper @ var_mu_hyper)[:, np.newaxis]\n",
    "    for i in range(dim):\n",
    "        factor[k][i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    return factor[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(error_tensor, pos_obs):\n",
    "    var_alpha = 1e-6 + 0.5 * len(pos_obs[0])\n",
    "    var_beta = 1e-6 + 0.5 * np.linalg.norm(error_tensor, 2) ** 2\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BATF with Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter):\n",
    "    \"\"\"Bayesian Augmented Tensor Factorization (BATF) with Gibbs sampling.\"\"\"\n",
    "\n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    rank = factor[0].shape[1]\n",
    "    if np.isnan(sparse_tensor).any() == False:\n",
    "        ind = sparse_tensor != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    elif np.isnan(sparse_tensor).any() == True:\n",
    "        pos_test = np.where((dense_tensor != 0) & (np.isnan(sparse_tensor)))\n",
    "        ind = ~np.isnan(sparse_tensor)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_tensor[np.isnan(sparse_tensor)] = 0\n",
    "    num_obs = len(pos_obs[0])\n",
    "    dense_test = dense_tensor[pos_test]\n",
    "    del dense_tensor\n",
    "\n",
    "    show_iter = 200\n",
    "    tau_eps = 1\n",
    "    bias = []\n",
    "    for k in range(len(dim)):\n",
    "        idx = tuple(filter(lambda x: x != k, range(len(dim))))\n",
    "        bias.append(np.sum(ind, axis = idx))\n",
    "    temp = cp_combine(factor)\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    tensor_hat_plus = np.zeros(dim)\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        temp = sparse_tensor - temp\n",
    "        mu_glb = sample_global_mu(temp[pos_obs] - vec_combine(vector)[pos_obs], pos_obs, tau_eps)\n",
    "        vector = sample_bias_vector(temp - mu_glb, factor, bias, ind, dim, k, tau_eps)\n",
    "        del temp\n",
    "        tau_sparse = tau_eps * ind * (sparse_tensor - mu_glb - vec_combine(vector))\n",
    "        for k in range(len(dim)):\n",
    "            factor[k] = sample_factor(tau_sparse, factor, ind, dim, k, tau_eps)\n",
    "        temp = cp_combine(factor)\n",
    "        tensor_hat = mu_glb + vec_combine(vector) + temp\n",
    "        temp_hat += tensor_hat[pos_test]\n",
    "        tau_eps = sample_precision_tau(sparse_tensor[pos_obs] - tensor_hat[pos_obs], pos_obs)\n",
    "        if it + 1 > burn_iter:\n",
    "            tensor_hat_plus += tensor_hat\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "    tensor_hat = tensor_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, tensor_hat[pos_test])))\n",
    "    print()\n",
    "\n",
    "    return tensor_hat, mu_glb, vector, factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Random Missing (RM)\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor'].transpose(0, 2, 1)\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, dim2, dim3) + 0.5 - missing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0831624\n",
      "RMSE: 3.58617\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0832097\n",
      "RMSE: 3.59167\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0830162\n",
      "RMSE: 3.585\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0829253\n",
      "RMSE: 3.58075\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0828773\n",
      "RMSE: 3.57861\n",
      "\n",
      "Imputation MAPE: 0.0828678\n",
      "Imputation RMSE: 3.57768\n",
      "\n",
      "Running time: 3509 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 80\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Random Missing (RM)\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor'].transpose(0, 2, 1)\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, dim2, dim3) + 0.5 - missing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.085489\n",
      "RMSE: 3.68523\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0855585\n",
      "RMSE: 3.69495\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.085521\n",
      "RMSE: 3.69398\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0854319\n",
      "RMSE: 3.69163\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0853651\n",
      "RMSE: 3.6893\n",
      "\n",
      "Imputation MAPE: 0.0853228\n",
      "Imputation RMSE: 3.68831\n",
      "\n",
      "Running time: 3398 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 80\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Non-random Missing (NM)\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor'].transpose(0, 2, 1)\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, dim3) + 0.5 - missing_rate)[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.10414\n",
      "RMSE: 4.34735\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.103932\n",
      "RMSE: 4.34829\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.103868\n",
      "RMSE: 4.34544\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.10384\n",
      "RMSE: 4.34298\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.103854\n",
      "RMSE: 4.34255\n",
      "\n",
      "Imputation MAPE: 0.10386\n",
      "Imputation RMSE: 4.34264\n",
      "\n",
      "Running time: 302 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Non-random Missing (NM)\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor'].transpose(0, 2, 1)\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, dim3) + 0.5 - missing_rate)[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.105701\n",
      "RMSE: 4.47143\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.106781\n",
      "RMSE: 4.52486\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.107459\n",
      "RMSE: 4.54978\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.107566\n",
      "RMSE: 4.55396\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.107503\n",
      "RMSE: 4.54894\n",
      "\n",
      "Imputation MAPE: 0.107488\n",
      "Imputation RMSE: 4.54743\n",
      "\n",
      "Running time: 273 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "dim1, dim2 = dense_mat.shape\n",
    "del binary_mat\n",
    "\n",
    "## Random missing (RM)\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "dense_tensor = dense_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "sparse_tensor = sparse_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.092024\n",
      "RMSE: 2.23937\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.091841\n",
      "RMSE: 2.23731\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0917828\n",
      "RMSE: 2.23639\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0917369\n",
      "RMSE: 2.23575\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0917014\n",
      "RMSE: 2.23519\n",
      "\n",
      "Imputation MAPE: 0.0916721\n",
      "Imputation RMSE: 2.23469\n",
      "\n",
      "Running time: 16042 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "dim1, dim2 = dense_mat.shape\n",
    "del binary_mat\n",
    "\n",
    "## Random missing (RM)\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "dense_tensor = dense_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "sparse_tensor = sparse_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0947566\n",
      "RMSE: 2.29987\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0945575\n",
      "RMSE: 2.29915\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0945481\n",
      "RMSE: 2.29784\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0945362\n",
      "RMSE: 2.29708\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0945147\n",
      "RMSE: 2.29646\n",
      "\n",
      "Imputation MAPE: 0.0945009\n",
      "Imputation RMSE: 2.29615\n",
      "\n",
      "Running time: 17420 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "dim1, dim2 = dense_mat.shape\n",
    "del binary_mat\n",
    "\n",
    "## Non-random missing (NM)\n",
    "dense_tensor = dense_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, 30) + 0.5 - missing_rate)[:, None, :]\n",
    "del dense_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0946398\n",
      "RMSE: 2.30881\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0945861\n",
      "RMSE: 2.31057\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0945354\n",
      "RMSE: 2.30897\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0944876\n",
      "RMSE: 2.30753\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0944528\n",
      "RMSE: 2.30685\n",
      "\n",
      "Imputation MAPE: 0.0944317\n",
      "Imputation RMSE: 2.30665\n",
      "\n",
      "Running time: 14074 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "dim1, dim2 = dense_mat.shape\n",
    "del binary_mat\n",
    "\n",
    "## Non-random missing (NM)\n",
    "dense_tensor = dense_mat.reshape([dim1, 30, 24]).transpose(0, 2, 1)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim1, 30) + 0.5 - missing_rate)[:, None, :]\n",
    "del dense_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0998295\n",
      "RMSE: 2.43301\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.099936\n",
      "RMSE: 2.44145\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0998842\n",
      "RMSE: 2.43961\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0999592\n",
      "RMSE: 2.44157\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0999222\n",
      "RMSE: 2.44101\n",
      "\n",
      "Imputation MAPE: 0.099921\n",
      "Imputation RMSE: 2.44112\n",
      "\n",
      "Running time: 13786 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on PeMS-4W Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_tensor = np.random.rand(data.values.shape[0], 288, 4 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_tensor = np.random.rand(data.values.shape[0], 288, 4 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 4 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_matrix, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 4 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_matrix, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on PeMS-8W Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_tensor = np.random.rand(data.values.shape[0], 288, 8 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_tensor = np.random.rand(data.values.shape[0], 288, 8 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 8 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_matrix, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_tensor = mat2ten(data.values, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 8 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "del data, random_matrix, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on London-1M Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)\n",
    "\n",
    "dense_tensor = dense_mat.reshape([dense_mat.shape[0], 30, 24])\n",
    "sparse_tensor = sparse_mat.reshape([sparse_mat.shape[0], 30, 24])\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)\n",
    "\n",
    "dense_tensor = dense_mat.reshape([dense_mat.shape[0], 30, 24])\n",
    "sparse_tensor = sparse_mat.reshape([sparse_mat.shape[0], 30, 24])\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)\n",
    "\n",
    "dense_tensor = dense_mat.reshape([dense_mat.shape[0], 30, 24])\n",
    "sparse_tensor = sparse_mat.reshape([sparse_mat.shape[0], 30, 24])\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)\n",
    "\n",
    "dense_tensor = dense_mat.reshape([dense_mat.shape[0], 30, 24])\n",
    "sparse_tensor = sparse_mat.reshape([sparse_mat.shape[0], 30, 24])\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 20\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Guangzhou-2M Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.08303\n",
      "RMSE: 3.58407\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.083102\n",
      "RMSE: 3.59169\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0830566\n",
      "RMSE: 3.59025\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0829992\n",
      "RMSE: 3.58897\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0829348\n",
      "RMSE: 3.58643\n",
      "\n",
      "Imputation MAPE: 0.0828811\n",
      "Imputation RMSE: 3.5845\n",
      "\n",
      "Running time: 3592 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 80\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0844221\n",
      "RMSE: 3.63857\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0843421\n",
      "RMSE: 3.64409\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.084293\n",
      "RMSE: 3.64176\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0842479\n",
      "RMSE: 3.63975\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.084262\n",
      "RMSE: 3.64037\n",
      "\n",
      "Imputation MAPE: 0.0842613\n",
      "Imputation RMSE: 3.63993\n",
      "\n",
      "Running time: 3612 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 80\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.101644\n",
      "RMSE: 4.29233\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.10161\n",
      "RMSE: 4.30334\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.101477\n",
      "RMSE: 4.30121\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.101412\n",
      "RMSE: 4.29953\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.101376\n",
      "RMSE: 4.29767\n",
      "\n",
      "Imputation MAPE: 0.101357\n",
      "Imputation RMSE: 4.29761\n",
      "\n",
      "Running time: 285 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor.shape)\n",
    "rank = 10\n",
    "vector = []\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    vector.append(0.1 * np.random.randn(dim[k],))\n",
    "    factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
