{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Tensor Factorization\n",
    "\n",
    "**Published**: December 27, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/predictor/BTTF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Tensor Factorization (BTTF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTTF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTTF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_u(tau_sparse_tensor, tau_ind, U, V, X, beta0 = 1):\n",
    "    \"\"\"Sampling M-by-R factor matrix U and its hyperparameters (mu_u, Lambda_u).\"\"\"\n",
    "    \n",
    "    dim1, rank = U.shape\n",
    "    U_bar = np.mean(U, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_mu_hyper = temp * U_bar\n",
    "    var_U_hyper = inv(np.eye(rank) + cov_mat(U, U_bar) + temp * beta0 * np.outer(U_bar, U_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_U_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim1 + beta0) * var_Lambda_hyper)\n",
    "\n",
    "    var1 = kr_prod(X, V).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_ind, 0).T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "    var4 = var1 @ ten2mat(tau_sparse_tensor, 0).T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "    for i in range(dim1):\n",
    "        U[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "        \n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_v(tau_sparse_tensor, tau_ind, U, V, X, beta0 = 1):\n",
    "    \"\"\"Sampling N-by-R factor matrix V and its hyperparameters (mu_v, Lambda_v).\"\"\"\n",
    "    \n",
    "    dim2, rank = V.shape\n",
    "    V_bar = np.mean(V, axis = 0)\n",
    "    temp = dim2 / (dim2 + beta0)\n",
    "    var_mu_hyper = temp * V_bar\n",
    "    var_V_hyper = inv(np.eye(rank) + cov_mat(V, V_bar) + temp * beta0 * np.outer(V_bar, V_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim2 + rank, scale = var_V_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim2 + beta0) * var_Lambda_hyper)\n",
    "\n",
    "    var1 = kr_prod(X, U).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_ind, 1).T).reshape([rank, rank, dim2]) + var_Lambda_hyper[:, :, None]\n",
    "    var4 = var1 @ ten2mat(tau_sparse_tensor, 1).T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "    for j in range(dim2):\n",
    "        V[j, :] = mvnrnd_pre(solve(var3[:, :, j], var4[:, j]), var3[:, :, j])\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_tensor, tau_ind, time_lags, U, V, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "\n",
    "    dim3, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = kr_prod(V, U).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_ind, 2).T).reshape([rank, rank, dim3]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ ten2mat(tau_sparse_tensor, 2).T\n",
    "    for t in range(dim3):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim3 - tmax and t < dim3 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim3))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim3 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar4cast(A, X, Sigma, time_lags, multi_step):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    X_new = np.append(X, np.zeros((multi_step, rank)), axis = 0)\n",
    "    for t in range(multi_step):\n",
    "        var = A.T @ X_new[dim + t - time_lags, :].reshape(rank * d)\n",
    "        X_new[dim + t, :] = mvnrnd(var, Sigma)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTTF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTTF(dense_tensor, sparse_tensor, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1):\n",
    "    \"\"\"Bayesian Temporal Tensor Factorization, BTTF.\"\"\"\n",
    "    \n",
    "    dim1, dim2, dim3 = sparse_tensor.shape\n",
    "    d = time_lags.shape[0]\n",
    "    U = init[\"U\"]\n",
    "    V = init[\"V\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_tensor).any() == False:\n",
    "        ind = sparse_tensor != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    elif np.isnan(sparse_tensor).any() == True:\n",
    "        pos_test = np.where((dense_tensor != 0) & (np.isnan(sparse_tensor)))\n",
    "        ind = ~np.isnan(sparse_tenosr)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_tensor[np.isnan(sparse_tensor)] = 0\n",
    "    dense_test = dense_tensor[pos_test]\n",
    "    del dense_tensor\n",
    "    U_plus = np.zeros((dim1, rank, gibbs_iter))\n",
    "    V_plus = np.zeros((dim2, rank, gibbs_iter))\n",
    "    X_plus = np.zeros((dim3 + multi_step, rank, gibbs_iter))\n",
    "    A_plus = np.zeros((rank * d, rank, gibbs_iter))\n",
    "    tau_plus = np.zeros(gibbs_iter)\n",
    "    Sigma_plus = np.zeros((rank, rank, gibbs_iter))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 500\n",
    "    tau = 1\n",
    "    tensor_hat_plus = np.zeros(sparse_tensor.shape)\n",
    "    tensor_new_plus = np.zeros((dim1, dim2, multi_step))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau * ind\n",
    "        tau_sparse_tensor = tau * sparse_tensor\n",
    "        U = sample_factor_u(tau_sparse_tensor, tau_ind, U, V, X)\n",
    "        V = sample_factor_v(tau_sparse_tensor, tau_ind, U, V, X)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_tensor, tau_ind, time_lags, U, V, X, A, inv(Sigma))\n",
    "        tensor_hat = np.einsum('is, js, ts -> ijt', U, V, X)\n",
    "        tau = np.random.gamma(1e-6 + 0.5 * np.sum(ind), \n",
    "                              1 / (1e-6 + 0.5 * np.sum(((sparse_tensor - tensor_hat) ** 2) * ind)))\n",
    "        temp_hat += tensor_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            U_plus[:, :, it - burn_iter] = U\n",
    "            V_plus[:, :, it - burn_iter] = V\n",
    "            A_plus[:, :, it - burn_iter] = A\n",
    "            Sigma_plus[:, :, it - burn_iter] = Sigma\n",
    "            tau_plus[it - burn_iter] = tau\n",
    "            tensor_hat_plus += tensor_hat\n",
    "            X0 = ar4cast(A, X, Sigma, time_lags, multi_step)\n",
    "            X_plus[:, :, it - burn_iter] = X0\n",
    "            tensor_new_plus += np.einsum('is, js, ts -> ijt', U, V, X0[- multi_step :, :])\n",
    "    tensor_hat = tensor_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, tensor_hat[:, :, : dim3][pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, tensor_hat[:, :, : dim3][pos_test])))\n",
    "    print()\n",
    "    tensor_hat = np.append(tensor_hat, tensor_new_plus / gibbs_iter, axis = 2)\n",
    "    tensor_hat[tensor_hat < 0] = 0\n",
    "    \n",
    "    return tensor_hat, U_plus, V_plus, X_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x_partial(tau_sparse_tensor, tau_ind, time_lags, U, V, X, A, Lambda_x, back_step):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim3, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = kr_prod(V, U).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_ind[:, :, - back_step :], 2).T).reshape([rank, rank, back_step]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ ten2mat(tau_sparse_tensor[:, :, - back_step :], 2).T\n",
    "    for t in range(dim3 - back_step, dim3):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim3 - tmax and t < dim3 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim3))[0]\n",
    "        if t < dim3 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        var3[:, :, t + back_step - dim3] = var3[:, :, t + back_step - dim3] + Mt\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t + back_step - dim3], \n",
    "                                   var4[:, t + back_step - dim3] + Nt + Qt), var3[:, :, t + back_step - dim3])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTTF_partial(dense_tensor, sparse_tensor, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1):\n",
    "    \"\"\"Bayesian Temporal Tensor Factorization, BTTF.\"\"\"\n",
    "    \n",
    "    dim1, dim2, dim3 = sparse_tensor.shape\n",
    "    U_plus = init[\"U_plus\"]\n",
    "    V_plus = init[\"V_plus\"]\n",
    "    X_plus = init[\"X_plus\"]\n",
    "    A_plus = init[\"A_plus\"]\n",
    "    Sigma_plus = init[\"Sigma_plus\"]\n",
    "    tau_plus = init[\"tau_plus\"]\n",
    "    if np.isnan(sparse_tensor).any() == False:\n",
    "        ind = sparse_tensor != 0\n",
    "        pos_obs = np.where(ind)\n",
    "    elif np.isnan(sparse_tensor).any() == True:\n",
    "        ind = ~np.isnan(sparse_tensor)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_tensor[np.isnan(sparse_tensor)] = 0\n",
    "    X_new_plus = np.zeros((dim3 + multi_step, rank, gibbs_iter))\n",
    "    tensor_new_plus = np.zeros((dim1, dim2, multi_step))\n",
    "    back_step = 10 * multi_step\n",
    "    for it in range(gibbs_iter):\n",
    "        tau_ind = tau_plus[it] * ind\n",
    "        tau_sparse_tensor = tau_plus[it] * sparse_tensor\n",
    "        X = sample_factor_x_partial(tau_sparse_tensor, tau_ind, time_lags, U_plus[:, :, it], V_plus[:, :, it],\n",
    "                                    X_plus[:, :, it], A_plus[:, :, it], inv(Sigma_plus[:, :, it]), back_step)\n",
    "        X0 = ar4cast(A_plus[:, :, it], X, Sigma_plus[:, :, it], time_lags, multi_step)\n",
    "        X_new_plus[:, :, it] = X0\n",
    "        tensor_new_plus += np.einsum('is, js, ts -> ijt', U_plus[:, :, it], V_plus[:, :, it], X0[- multi_step :, :])\n",
    "    tensor_hat = tensor_new_plus / gibbs_iter\n",
    "    tensor_hat[tensor_hat < 0] = 0\n",
    "    \n",
    "    return tensor_hat, U_plus, V_plus, X_new_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "def BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter):\n",
    "    dim1, dim2, T = dense_tensor.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    tensor_hat = np.zeros((dim1, dim2, max_count * multi_step))\n",
    "    f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"U\": 0.1 * np.random.randn(dim1, rank), \n",
    "                    \"V\": 0.1 * np.random.randn(dim2, rank),\n",
    "                    \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            tensor, U, V, X_new, A, Sigma, tau = BTTF(dense_tensor[:, :, : start_time], \n",
    "                sparse_tensor[:, :, : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step)\n",
    "        else:\n",
    "            init = {\"U_plus\": U, \"V_plus\": V, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            tensor, U, V, X_new, A, Sigma, tau = BTTF_partial(dense_tensor[:, :, : start_time + t * multi_step], \n",
    "                sparse_tensor[:, :, : start_time + t * multi_step], init, rank, time_lags, burn_iter, gibbs_iter, multi_step)\n",
    "        tensor_hat[:, :, t * multi_step : (t + 1) * multi_step] = tensor[:, :, - multi_step :]\n",
    "        f.value = t\n",
    "    small_dense_tensor = dense_tensor[:, :, start_time : T]\n",
    "    pos = np.where(small_dense_tensor != 0)\n",
    "    print('Prediction MAPE: {:.6}'.format(compute_mape(small_dense_tensor[pos], tensor_hat[pos])))\n",
    "    print('Prediction RMSE: {:.6}'.format(compute_rmse(small_dense_tensor[pos], tensor_hat[pos])))\n",
    "    print()\n",
    "    return tensor_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eavluation on NYC Taxi Flow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Test on original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "rm_tensor = scipy.io.loadmat('../datasets/NYC-data-set/rm_tensor.mat')['rm_tensor']\n",
    "sparse_tensor = dense_tensor.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time horizon (delta) = 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fba4bf54f04b36945c3d0fdd2a0829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=84)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "rm_tensor = scipy.io.loadmat('../datasets/NYC-data-set/rm_tensor.mat')['rm_tensor']\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(rm_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "rm_tensor = scipy.io.loadmat('../datasets/NYC-data-set/rm_tensor.mat')['rm_tensor']\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(rm_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "nm_tensor = scipy.io.loadmat('../datasets/NYC-data-set/nm_tensor.mat')['nm_tensor']\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        for i3 in range(61):\n",
    "            binary_tensor[i1, i2, i3 * 24 : (i3 + 1) * 24] = np.round(nm_tensor[i1, i2, i3] + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Pacific Surface Temperature Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Test on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "dense_tensor[np.where(dense_tensor == 124)] = 0 # Outliers\n",
    "sparse_tensor = dense_tensor.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 10 * 12\n",
    "time_lags = np.array([1, 2, 3, 12, 13, 14, 2 * 12, 2 * 12 + 1, 2 * 12 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "dense_tensor[np.where(dense_tensor == 124)] = 0 # Outliers\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], dense_tensor.shape[2])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 10 * 12\n",
    "time_lags = np.array([1, 2, 3, 12, 13, 14, 2 * 12, 2 * 12 + 1, 2 * 12 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "dense_tensor[np.where(dense_tensor == 124)] = 0 # Outliers\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], dense_tensor.shape[2])\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 10 * 12\n",
    "time_lags = np.array([1, 2, 3, 12, 13, 14, 2 * 12, 2 * 12 + 1, 2 * 12 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "dense_tensor[np.where(dense_tensor == 124)] = 0 # Outliers\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], int(dense_tensor.shape[2] / 3))\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        for i3 in range(int(dense_tensor.shape[2] / 3)):\n",
    "            binary_tensor[i1, i2, i3 * 3 : (i3 + 1) * 3] = np.round(random_tensor[i1, i2, i3] + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 30\n",
    "pred_step = 10 * 12\n",
    "time_lags = np.array([1, 2, 3, 12, 13, 14, 2 * 12, 2 * 12 + 1, 2 * 12 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    tensor_hat = BTTF_forecast(dense_tensor, sparse_tensor, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
