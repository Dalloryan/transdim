{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Revised**: October 8, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Matrix Factorization (BTMF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTMF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTMF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Large-scale and multidimensional spatiotemporal data sets are becoming ubiquitous in many real-world applications such as monitoring traffic and air quality. Making predictions on these time series has become a critical challenge due to not only the large-scale and high-dimensional nature but also the considerable amount of missing data. In this work, we propose a Bayesian Temporal Matrix Factorization (BTMF) model for modeling multidimensional time series - and in particular spatiotemporal data - in the presence of missing values. By integrating low-rank matrix factorization and vector autoregressive (VAR) process into a single probabilistic graphical model, our model can effectively perform predictions without imputing those missing values. We develop efficient Gibbs sampling algorithms for model inference and test BTMF on several real-world spatiotemporal data sets (i.e., a typical kind of multivariate time series data) for both missing data imputation and short-term rolling prediction tasks. This post is mainly about BTMF and its **`Python`** implementation with an application of spatiotemporal data imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "We assume a spatiotemporal setting for multidimensional time series data throughout this work. In general, modern spatiotemporal data sets collected from sensor networks can be organized as matrix time series. For example, we can denote by matrix $Y\\in\\mathbb{R}^{N\\times T}$ a multivariate time series collected from $N$ locations/sensors on $T$ time points, with each row $$\\boldsymbol{y}_{i}=\\left(y_{i,1},y_{i,2},...,y_{i,t-1},y_{i,t},y_{i,t+1},...,y_{i,T}\\right)$$\n",
    "corresponding to the time series collected at location $i$.\n",
    "\n",
    "As mentioned, making accurate predictions on incomplete time series is very challenging, while missing data problem is almost inevitable in real-world applications. Figure 1 illustrates the prediction problem for incomplete time series data. Here we use $(i,t)\\in\\Omega$ to index the observed entries in matrix $Y$.\n",
    "\n",
    "<img src=\"../images/graphical_matrix_time_series.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "> **Figure 1**: Illustration of multivariate time series and the prediction problem in the presence of missing values (green: observed data; white: missing data; red: prediction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "Given a partially observed spatiotemporal matrix $Y\\in\\mathbb{R}^{N \\times T}$, one can factorize it into a spatial factor matrix $W\\in\\mathbb{R}^{R \\times N}$ and a temporal factor matrix $X\\in\\mathbb{R}^{R \\times T}$ following general matrix factorization model:\n",
    "\\begin{equation}\n",
    "Y\\approx W^{\\top}X,\n",
    "\\label{btmf_equation1}\n",
    "\\end{equation}\n",
    "and element-wise, we have\n",
    "\\begin{equation}\n",
    "y_{it}\\approx \\boldsymbol{w}_{i}^\\top\\boldsymbol{x}_{t}, \\quad \\forall (i,t),\n",
    "\\label{btmf_equation2}\n",
    "\\end{equation}\n",
    "where vectors $\\boldsymbol{w}_{i}$ and $\\boldsymbol{x}_{t}$ refer to the $i$-th column of $W$ and the $t$-th column of $X$, respectively.\n",
    "\n",
    "The standard matrix factorization model is a good approach to deal with the missing data problem; however, it cannot capture the dependencies among different columns in $X$, which are critical in modeling time series data. To better characterize the temporal dependencies and impose temporal smoothness, a novel AR regularizer is introduced on $X$ in TRMF (i.e., Temporal Regularizer Matrix Factorization proposed by [Yu et al., 2016](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf)):\n",
    "\\begin{equation} \\label{equ:VAR}\n",
    "\\begin{aligned}\n",
    "    \\boldsymbol{x}_{t+1}&=\\sum\\nolimits_{k=1}^{d}A_{k}\\boldsymbol{x}_{t+1-h_k}+\\boldsymbol{\\epsilon}_t, \\\\\n",
    "    &=A^\\top \\boldsymbol{v}_{t+1}+\\boldsymbol{\\epsilon}_{t}, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\mathcal{L}=\\left\\{h_1,\\ldots,h_k,\\ldots,h_d\\right\\}$ is a lag set ($d$ is the order of this AR model), each $A_k$ ($k\\in\\left\\{1,...,d\\right\\}$) is a $R\\times R$ coefficient matrix, and $\\boldsymbol{\\epsilon}_t$ is a zero mean Gaussian noise vector. For brevity, matrix $A\\in \\mathbb{R}^{(R d) \\times R}$ and vector $\\boldsymbol{v}_{t+1}\\in \\mathbb{R}^{(R d) \\times 1}$ are defined as\n",
    "\\begin{equation*}\n",
    "A=\\left[A_{1}, \\ldots, A_{d}\\right]^{\\top} ,\\quad \\boldsymbol{v}_{t+1}=\\left[\\begin{array}{c}{\\boldsymbol{x}_{t+1-h_1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{t+1-h_d}}\\end{array}\\right] .\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\"../images/rolling_prediction.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "> **Figure 2**: A graphical illustration of the rolling prediction scheme using BTMF (with VAR process) (green: observed data; white: missing data; red: prediction).\n",
    "\n",
    "In [Yu et al., 2016](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf), to avoid overfitting and reduce the number of parameters, the coefficient matrix in TRMF is further assumed to be a diagonal $A_k=\\text{diag}(\\boldsymbol{\\theta}_{k})$. Therefore, they have\n",
    "\\begin{equation} \\label{equ:AR}\n",
    "\\boldsymbol{x}_{t+1}=\\boldsymbol{\\theta}_{1}\\circledast\\boldsymbol{x}_{t+1-h_1}+\\cdots+\\boldsymbol{\\theta}_{d}\\circledast\\boldsymbol{x}_{t+1-h_d}+\\boldsymbol{\\epsilon}_t,\n",
    "\\end{equation}\n",
    "where the symbol $\\circledast$ denotes the element-wise Hadamard product. However, unlike this individual autoregressive (AR) process, a vector autoregressive (VAR) process is actually more powerful for capturing multivariate time series patterns.\n",
    "\n",
    "<img src=\"../images/rolling_prediction_strategy.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "> **Figure 3**: A graphical illustration of the rolling prediction scheme using BTMF (with AR process) (green: observed data; white: missing data; red: prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Temporal Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Model Specification\n",
    "\n",
    "Following the general Bayesian probabilistic matrix factorization models (e.g., BPMF proposed by [Salakhutdinov & Mnih, 2008](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf)), we assume that each observed entry in $Y$ follows a Gaussian distribution with precision $\\tau$:\n",
    "\\begin{equation}\n",
    "y_{i,t}\\sim\\mathcal{N}\\left(\\boldsymbol{w}_i^\\top\\boldsymbol{x}_t,\\tau^{-1}\\right),\\quad \\left(i,t\\right)\\in\\Omega.\n",
    "\\label{btmf_equation3}\n",
    "\\end{equation}\n",
    "\n",
    "On the spatial dimension, we use a simple Gaussian factor matrix without imposing any dependencies explicitly:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{w}_i\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{w},\\Lambda_w^{-1}\\right),\n",
    "\\end{equation}\n",
    "and we place a conjugate Gaussian-Wishart prior on the mean vector and the precision matrix:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mu}_w | \\Lambda_w \\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,(\\beta_0\\Lambda_w)^{-1}\\right),\\Lambda_w\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{\\mu}_0\\in \\mathbb{R}^{R}$ is a mean vector, $\\mathcal{W}\\left(W_0,\\nu_0\\right)$ is a Wishart distribution with a $R\\times R$ scale matrix $W_0$ and $\\nu_0$ degrees of freedom.\n",
    "\n",
    "In modeling the temporal factor matrix $X$, we re-write the VAR process as:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\n",
    "\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),&\\text{if $t\\in\\left\\{1,2,...,h_d\\right\\}$}, \\\\\n",
    "\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),&\\text{otherwise},\\\\\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "\\label{btmf_equation5}\n",
    "\\end{equation}\n",
    "\n",
    "Since the mean vector is defined by VAR, we need to place the conjugate matrix normal inverse Wishart (MNIW) prior on the coefficient matrix $A$ and the covariance matrix $\\Sigma$ as follows,\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "A\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right),\\quad\n",
    "\\Sigma \\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where the probability density function for the $Rd$-by-$R$ random matrix $A$ has the form:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&p\\left(A\\mid M_0,\\Psi_0,\\Sigma\\right) \\\\\n",
    "=&\\left(2\\pi\\right)^{-R^2d/2}\\left|\\Psi_0\\right|^{-R/2}\\left|\\Sigma\\right|^{-Rd/2} \\\\\n",
    "&\\times \\exp\\left(-\\frac{1}{2}\\text{tr}\\left[\\Sigma^{-1}\\left(A-M_0\\right)^{\\top}\\Psi_{0}^{-1}\\left(A-M_0\\right)\\right]\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\label{mnpdf}\n",
    "\\end{equation}\n",
    "where $\\Psi_0\\in\\mathbb{R}^{(Rd)\\times (Rd)}$ and $\\Sigma\\in\\mathbb{R}^{R\\times R}$ are played as covariance matrices.\n",
    "\n",
    "For the only remaining parameter $\\tau$, we place a Gamma prior  $\\tau\\sim\\text{Gamma}\\left(\\alpha,\\beta\\right)$ where $\\alpha$ and $\\beta$ are the shape and rate parameters, respectively. \n",
    "\n",
    "The above specifies the full generative process of BTMF, and we could also see the Bayesian graphical model shown in Figure 4. Several parameters are introduced to define the prior distributions for hyperparameters, including $\\boldsymbol{\\mu}_{0}$, $W_0$, $\\nu_0$, $\\beta_0$, $\\alpha$, $\\beta$, $M_0$, $\\Psi_0$, and $S_0$. These parameters need to provided in advance when training the model. However, it should be noted that the specification of these parameters has little impact on the final results, as the training data will play a much more important role in defining the posteriors of the hyperparameters.\n",
    "\n",
    "<img src=\"../images/btmf_net.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "> **Figure 4**: An overview graphical model of BTMF (time lag set: $\\left\\{1,2,...,d\\right\\}$). The shaded nodes ($y_{i,t}$) are the observed data in $\\Omega$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Model Inference\n",
    "\n",
    "Given the complex structure of BTMF, it is intractable to write down the posterior distribution. Here we rely on the MCMC technique for Bayesian learning. In detail, we introduce a Gibbs sampling algorithm by deriving the full conditional distributions for all parameters and hyperparameters. Thanks to the use of conjugate priors in Figure 4, we can actually write down all the conditional distributions analytically. Below we summarize the Gibbs sampling procedure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Sampling Factor Matrix $W$ and Its Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For programming convenience, we use $W\\in\\mathbb{R}^{N\\times R}$ to replace $W\\in\\mathbb{R}^{R\\times N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(temp * W_bar, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Sampling VAR Coefficients $A$ and Its Hyperparameters\n",
    "\n",
    "**Foundations of VAR**\n",
    "\n",
    "Vector autoregression (VAR) is a multivariate extension of autoregression (AR). Formally, VAR for $R$-dimensional vectors $\\boldsymbol{x}_{t}$ can be written as follows,\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&=A_{1} \\boldsymbol{x}_{t-h_1}+\\cdots+A_{d} \\boldsymbol{x}_{t-h_d}+\\boldsymbol{\\epsilon}_{t}, \\\\\n",
    "&= A^\\top \\boldsymbol{v}_{t}+\\boldsymbol{\\epsilon}_{t},~t=h_d+1, \\ldots, T, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "A=\\left[A_{1}, \\ldots, A_{d}\\right]^{\\top} \\in \\mathbb{R}^{(R d) \\times R},\\quad \\boldsymbol{v}_{t}=\\left[\\begin{array}{c}{\\boldsymbol{x}_{t-h_1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{t-h_d}}\\end{array}\\right] \\in \\mathbb{R}^{(R d) \\times 1}.\n",
    "\\end{equation}\n",
    "\n",
    "In the following, if we define\n",
    "\\begin{equation}\n",
    "Z=\\left[\\begin{array}{c}{\\boldsymbol{x}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-h_d) \\times R},\\quad Q=\\left[\\begin{array}{c}{\\boldsymbol{v}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{v}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-h_d) \\times(R d)},\n",
    "\\end{equation}\n",
    "then, we could write the above mentioned VAR as\n",
    "\\begin{equation}\n",
    "\\underbrace{Z}_{(T-h_d)\\times R}\\approx \\underbrace{Q}_{(T-h_d)\\times (Rd)}\\times \\underbrace{A}_{(Rd)\\times R}.\n",
    "\\end{equation}\n",
    "\n",
    "> To include temporal factors $\\boldsymbol{x}_{t},t=1,...,h_d$, we also define $$Z_0=\\left[\\begin{array}{c}{\\boldsymbol{x}_{1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{h_d}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{h_d \\times R}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Bayesian VAR on temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "A&\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right), \\\\\n",
    "\\Sigma &\\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{M N}_{(R d) \\times R}\\left(A | M_{0}, \\Psi_{0}, \\Sigma\\right)\\\\\n",
    "\\propto|&\\Sigma|^{-R d / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)\\right]\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\mathcal{I} \\mathcal{W}\\left(\\Sigma | S_{0}, \\nu_{0}\\right) \\propto|\\Sigma|^{-\\left(\\nu_{0}+R+1\\right) / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(\\Sigma^{-1}S_{0}\\right)\\right).\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood from temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{L}\\left(X\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\prod_{t=1}^{h_d}p\\left(\\boldsymbol{x}_{t}\\mid \\Sigma\\right)\\times \\prod_{t=h_d+1}^{T}p\\left(\\boldsymbol{x}_{t}\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\sum_{t=h_d+1}^{T}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)^\\top\\Sigma^{-1}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)\\right\\} \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\text{tr}\\left[\\Sigma^{-1}\\left(Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right)\\right)\\right]\\right\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior distribution**\n",
    "\n",
    "Consider\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)+S_0+Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right) \\\\\n",
    "=&A^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)A-A^\\top\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top A \\\\\n",
    "&+\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&+M_0^\\top\\Psi_0^{-1}M_0+S_0+Z_0^\\top Z_0+Z^\\top Z \\\\\n",
    "=&\\left(A-M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}\\left(A-M^{*}\\right)+S^{*}, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "which is in the form of $\\mathcal{MN}\\left(\\cdot\\right)$ and $\\mathcal{IW}\\left(\\cdot\\right)$.\n",
    "\n",
    "The $Rd$-by-$R$ matrix $A$ has a matrix normal distribution, and $R$-by-$R$ covariance matrix $\\Sigma$ has an inverse Wishart distribution, that is,\n",
    "\\begin{equation}\n",
    "A \\sim \\mathcal{M N}_{(R d) \\times R}\\left(M^{*}, \\Psi^{*}, \\Sigma\\right), \\quad \\Sigma \\sim \\mathcal{I} \\mathcal{W}\\left(S^{*}, \\nu^{*}\\right),\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "{\\Psi^{*}=\\left(\\Psi_{0}^{-1}+Q^{\\top} Q\\right)^{-1}}, \\\\ {M^{*}=\\Psi^{*}\\left(\\Psi_{0}^{-1} M_{0}+Q^{\\top} Z\\right)}, \\\\ {S^{*}=S_{0}+Z^\\top Z+M_0^\\top\\Psi_0^{-1}M_0-\\left(M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}M^{*}}, \\\\ \n",
    "{\\nu^{*}=\\nu_{0}+T-h_d}.\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Sampling Factor Matrix $X$\n",
    "\n",
    "**Posterior distribution**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y_{it}&\\sim\\mathcal{N}\\left(\\boldsymbol{w}_{i}^\\top\\boldsymbol{x}_{t},\\tau^{-1}\\right),~\\left(i,t\\right)\\in\\Omega, \\\\\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(\\sum_{k=1}^{d}A_{k} \\boldsymbol{x}_{t-h_k},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{1,...,h_d\\right\\}$, parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+I\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}\\right). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{h_d+1,...,T\\right\\}$, then parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+\\Sigma^{-1}\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}+\\Sigma^{-1}\\sum_{k=1}^{d}A_{k}\\boldsymbol{x}_{t-h_k}\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "$$\\boldsymbol{\\psi}_{t+h_k}=\\boldsymbol{x}_{t+h_k}-\\sum_{l=1,l\\neq k}^{d}A_{l}\\boldsymbol{x}_{t+h_k-h_l}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Sampling Precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "def sample_precision_scalar_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) BTMF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_plus = np.zeros((dim2, rank))\n",
    "    A_plus = np.zeros((rank * d, rank))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 200\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        if option == \"factor\":\n",
    "            tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        elif option == \"pca\":\n",
    "            tau = sample_precision_scalar_tau(sparse_mat, mat_hat, ind)\n",
    "            tau = tau * np.ones(dim1)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus += W\n",
    "            X_plus += X\n",
    "            A_plus += A\n",
    "            mat_hat_plus += mat_hat\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    W = W_plus / gibbs_iter\n",
    "    X = X_plus / gibbs_iter\n",
    "    A = A_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print()\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W, X, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Guangzhou Speed Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Non-random missing (NM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1])[:, :, np.newaxis] + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Time lags: {1, 2, 144}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.104269\n",
      "RMSE: 4.36081\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.101626\n",
      "RMSE: 4.30256\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.101615\n",
      "RMSE: 4.3037\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.101683\n",
      "RMSE: 4.30592\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.101673\n",
      "RMSE: 4.30681\n",
      "\n",
      "Imputation MAPE: 0.101755\n",
      "Imputation RMSE: 4.30853\n",
      "\n",
      "Running time: 933 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 80\n",
    "- Time lags: {1, 2, 144}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0778319\n",
      "RMSE: 3.33635\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0768004\n",
      "RMSE: 3.31004\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0764412\n",
      "RMSE: 3.30118\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.076182\n",
      "RMSE: 3.29244\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0761008\n",
      "RMSE: 3.29149\n",
      "\n",
      "Imputation MAPE: 0.0760784\n",
      "Imputation RMSE: 3.29119\n",
      "\n",
      "Running time: 6351 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.6 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 80\n",
    "- Time lags: {1, 2, 144}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0842265\n",
      "RMSE: 3.59357\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0831631\n",
      "RMSE: 3.55573\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0826056\n",
      "RMSE: 3.53509\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0822812\n",
      "RMSE: 3.52181\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0820294\n",
      "RMSE: 3.51294\n",
      "\n",
      "Imputation MAPE: 0.0819592\n",
      "Imputation RMSE: 3.51001\n",
      "\n",
      "Running time: 6355 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Birmingham Parking Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 77\\times 18$ (parking slot, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Birmingham-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Non-random missing (NM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1])[:, :, np.newaxis] + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 18}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.160954\n",
      "RMSE: 99.1409\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.130435\n",
      "RMSE: 80.5966\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.133369\n",
      "RMSE: 80.2392\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.127632\n",
      "RMSE: 77.7049\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.133299\n",
      "RMSE: 74.0348\n",
      "\n",
      "Imputation MAPE: 0.13379\n",
      "Imputation RMSE: 69.5338\n",
      "\n",
      "Running time: 204 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 18])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 77\\times 18$ (parking slot, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Birmingham-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 18}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0437143\n",
      "RMSE: 21.7522\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0356367\n",
      "RMSE: 17.9024\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0345634\n",
      "RMSE: 17.0658\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0340878\n",
      "RMSE: 16.2093\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0347393\n",
      "RMSE: 16.63\n",
      "\n",
      "Imputation MAPE: 0.0343784\n",
      "Imputation RMSE: 18.1942\n",
      "\n",
      "Running time: 204 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 18])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 77\\times 18$ (parking slot, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Birmingham-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.6 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 18}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0832529\n",
      "RMSE: 64.7848\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0700006\n",
      "RMSE: 33.6699\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0663554\n",
      "RMSE: 27.0075\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0629423\n",
      "RMSE: 26.593\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0619536\n",
      "RMSE: 27.0718\n",
      "\n",
      "Imputation MAPE: 0.0616117\n",
      "Imputation RMSE: 25.9018\n",
      "\n",
      "Running time: 204 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 18])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Hangzhou Flow Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Non-random missing (NM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1])[:, :, np.newaxis] + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 108}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.280562\n",
      "RMSE: 103.909\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.27785\n",
      "RMSE: 84.921\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.267921\n",
      "RMSE: 68.0001\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.258384\n",
      "RMSE: 58.105\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.259447\n",
      "RMSE: 48.4835\n",
      "\n",
      "Imputation MAPE: 0.259309\n",
      "Imputation RMSE: 46.0055\n",
      "\n",
      "Running time: 626 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 108])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 108}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.230024\n",
      "RMSE: 41.7666\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.237999\n",
      "RMSE: 34.9787\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.238276\n",
      "RMSE: 31.4654\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.237635\n",
      "RMSE: 30.5488\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.238353\n",
      "RMSE: 30.1015\n",
      "\n",
      "Imputation MAPE: 0.239989\n",
      "Imputation RMSE: 30.0307\n",
      "\n",
      "Running time: 597 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 108])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.6 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 108}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.241568\n",
      "RMSE: 47.9988\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.253166\n",
      "RMSE: 40.6155\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.260333\n",
      "RMSE: 37.0129\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.26306\n",
      "RMSE: 36.3198\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.255018\n",
      "RMSE: 35.4753\n",
      "\n",
      "Imputation MAPE: 0.251851\n",
      "Imputation RMSE: 35.2499\n",
      "\n",
      "Running time: 598 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 108])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Seattle Speed Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Seattle-data-set/tensor.npz')['arr_0']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Non-random missing (NM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1])[:, :, np.newaxis] + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Time lags: {1, 2, 288}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0963432\n",
      "RMSE: 5.48416\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0926811\n",
      "RMSE: 5.34828\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0926641\n",
      "RMSE: 5.3471\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0926791\n",
      "RMSE: 5.34896\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0926937\n",
      "RMSE: 5.349\n",
      "\n",
      "Imputation MAPE: 0.0927048\n",
      "Imputation RMSE: 5.35061\n",
      "\n",
      "Running time: 1015 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Seattle-data-set/tensor.npz')['arr_0']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 50\n",
    "- Time lags: {1, 2, 288}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.060615\n",
      "RMSE: 3.78165\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0598819\n",
      "RMSE: 3.76034\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0596581\n",
      "RMSE: 3.75081\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.059572\n",
      "RMSE: 3.74568\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.05955\n",
      "RMSE: 3.7446\n",
      "\n",
      "Imputation MAPE: 0.059578\n",
      "Imputation RMSE: 3.74506\n",
      "\n",
      "Running time: 2911 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 50\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Seattle-data-set/tensor.npz')['arr_0']\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.6 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "dense_mat = dense_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "sparse_mat = sparse_tensor.reshape([dim[0], dim[1] * dim[2]])\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 50\n",
    "- Time lags: {1, 2, 288}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0634663\n",
      "RMSE: 3.9178\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0619926\n",
      "RMSE: 3.8534\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.061864\n",
      "RMSE: 3.84633\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0617191\n",
      "RMSE: 3.84037\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0616136\n",
      "RMSE: 3.83723\n",
      "\n",
      "Imputation MAPE: 0.0615285\n",
      "Imputation RMSE: 3.83463\n",
      "\n",
      "Running time: 2920 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 50\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on London Movement Speed Data\n",
    "\n",
    "London movement speed data set is is a city-wide hourly traffic speeddataset collected in London.\n",
    "\n",
    "- Collected from 200,000+ road segments.\n",
    "- 720 time points in April 2019.\n",
    "- 73% missing values in the original data.\n",
    "\n",
    "|  Observation rate | $>90\\%$ | $>80\\%$ | $>70\\%$ | $>60\\%$ | $>50\\%$ |\n",
    "|:------------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|**Number of roads**|  17,666 |  27,148 |  35,912 |  44,352 |  52,727 |\n",
    "\n",
    "\n",
    "If want to test on the full dataset, you could consider the following setting for masking observations as missing values. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "mask_rate = 0.20\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "pos_obs = np.where(dense_mat != 0)\n",
    "num = len(pos_obs[0])\n",
    "sample_ind = np.random.choice(num, size = int(mask_rate * num), replace = False)\n",
    "sparse_mat = dense_mat.copy()\n",
    "sparse_mat[pos_obs[0][sample_ind], pos_obs[1][sample_ind]] = 0\n",
    "```\n",
    "\n",
    "Notably, you could also consider to evaluate the model on a subset of the data with the following setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.4\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = dense_mat * binary_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.101369\n",
      "RMSE: 2.52938\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0943416\n",
      "RMSE: 2.32419\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0942804\n",
      "RMSE: 2.32234\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0942548\n",
      "RMSE: 2.32151\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0942522\n",
      "RMSE: 2.32131\n",
      "\n",
      "Imputation MAPE: 0.0942448\n",
      "Imputation RMSE: 2.32114\n",
      "\n",
      "Running time: 3434 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.4\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = dense_mat * binary_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.101091\n",
      "RMSE: 2.53104\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0915506\n",
      "RMSE: 2.26009\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0915279\n",
      "RMSE: 2.25953\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0915008\n",
      "RMSE: 2.25885\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0914937\n",
      "RMSE: 2.25851\n",
      "\n",
      "Imputation MAPE: 0.0914913\n",
      "Imputation RMSE: 2.25831\n",
      "\n",
      "Running time: 3707 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.6\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = dense_mat * binary_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 20\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.118065\n",
      "RMSE: 2.97448\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0933272\n",
      "RMSE: 2.30121\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0933082\n",
      "RMSE: 2.30072\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0933009\n",
      "RMSE: 2.30048\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.093254\n",
      "RMSE: 2.29921\n",
      "\n",
      "Imputation MAPE: 0.0931998\n",
      "Imputation RMSE: 2.29791\n",
      "\n",
      "Running time: 3668 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eavluation on NYC Taxi Flow Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.4 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.438729\n",
      "RMSE: 4.51639\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.441871\n",
      "RMSE: 4.58073\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.442271\n",
      "RMSE: 4.58424\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.442407\n",
      "RMSE: 4.58285\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.442621\n",
      "RMSE: 4.58954\n",
      "\n",
      "Imputation MAPE: 0.442656\n",
      "Imputation RMSE: 4.5902\n",
      "\n",
      "Running time: 576 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor'].astype(np.float32)\n",
    "dim = dense_tensor.shape\n",
    "missing_rate = 0.6 # Random missing (RM)\n",
    "sparse_tensor = dense_tensor * np.round(np.random.rand(dim[0], dim[1], dim[2]) + 0.5 - missing_rate)\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.449161\n",
      "RMSE: 4.69922\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.454175\n",
      "RMSE: 4.77746\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.455749\n",
      "RMSE: 4.81528\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.455906\n",
      "RMSE: 4.81052\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.456352\n",
      "RMSE: 4.81612\n",
      "\n",
      "Imputation MAPE: 0.456358\n",
      "Imputation RMSE: 4.81239\n",
      "\n",
      "Running time: 498 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 30\\times 1461$ (origin, destination, time)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = scipy.io.loadmat('../datasets/NYC-data-set/tensor.mat')['tensor']\n",
    "dim = dense_tensor.shape\n",
    "nm_tensor = np.random.rand(dim[0], dim[1], dim[2])\n",
    "missing_rate = 0.4 # Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dim[0]):\n",
    "    for i2 in range(dim[1]):\n",
    "        for i3 in range(61):\n",
    "            binary_tensor[i1, i2, i3 * 24 : (i3 + 1) * 24] = np.round(nm_tensor[i1, i2, i3] + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor * binary_tensor\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]\n",
    "del dense_tensor, sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 24}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.444464\n",
      "RMSE: 4.65518\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.447948\n",
      "RMSE: 4.75414\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.448308\n",
      "RMSE: 4.75636\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.448359\n",
      "RMSE: 4.7692\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.448241\n",
      "RMSE: 4.76313\n",
      "\n",
      "Imputation MAPE: 0.448418\n",
      "Imputation RMSE: 4.77218\n",
      "\n",
      "Running time: 517 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Pacific Surface Temperature Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "pos = np.where(dense_tensor[:, 0, :] > 50)\n",
    "dense_tensor[pos[0], :, pos[1]] = 0\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], dense_tensor.shape[2])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 12}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0137389\n",
      "RMSE: 0.457494\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0137986\n",
      "RMSE: 0.45871\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0137956\n",
      "RMSE: 0.458598\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0137929\n",
      "RMSE: 0.45853\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0137936\n",
      "RMSE: 0.458537\n",
      "\n",
      "Imputation MAPE: 0.0137925\n",
      "Imputation RMSE: 0.458508\n",
      "\n",
      "Running time: 344 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 12])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "pos = np.where(dense_tensor[:, 0, :] > 50)\n",
    "dense_tensor[pos[0], :, pos[1]] = 0\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], dense_tensor.shape[2])\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 12}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.014609\n",
      "RMSE: 0.489699\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0145158\n",
      "RMSE: 0.48626\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0145027\n",
      "RMSE: 0.485859\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0145018\n",
      "RMSE: 0.485809\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0144967\n",
      "RMSE: 0.485647\n",
      "\n",
      "Imputation MAPE: 0.0144962\n",
      "Imputation RMSE: 0.485584\n",
      "\n",
      "Running time: 358 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 12])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $30\\times 84\\times 396$ (location x, location y, month)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_tensor = np.load('../datasets/Temperature-data-set/tensor.npy').astype(np.float32)\n",
    "pos = np.where(dense_tensor[:, 0, :] > 50)\n",
    "dense_tensor[pos[0], :, pos[1]] = 0\n",
    "random_tensor = np.random.rand(dense_tensor.shape[0], dense_tensor.shape[1], int(dense_tensor.shape[2] / 3))\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        for i3 in range(int(dense_tensor.shape[2] / 3)):\n",
    "            binary_tensor[i1, i2, i3 * 3 : (i3 + 1) * 3] = np.round(random_tensor[i1, i2, i3] + 0.5 - missing_rate)\n",
    "sparse_tensor = dense_tensor.copy()\n",
    "sparse_tensor[binary_tensor == 0] = np.nan\n",
    "sparse_tensor[sparse_tensor == 0] = np.nan\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "dense_mat = np.zeros((dim1 * dim2, dim3))\n",
    "sparse_mat = np.zeros((dim1 * dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    dense_mat[i * dim2 : (i + 1) * dim2, :] = dense_tensor[i, :, :]\n",
    "    sparse_mat[i * dim2 : (i + 1) * dim2, :] = sparse_tensor[i, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 30\n",
    "- Time lags: {1, 2, 12}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.0144348\n",
      "RMSE: 0.478619\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.0144376\n",
      "RMSE: 0.478621\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.0144356\n",
      "RMSE: 0.478508\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.0144289\n",
      "RMSE: 0.478346\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.0144225\n",
      "RMSE: 0.478176\n",
      "\n",
      "Imputation MAPE: 0.0144058\n",
      "Imputation RMSE: 0.477715\n",
      "\n",
      "Running time: 354 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 30\n",
    "time_lags = np.array([1, 2, 12])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"pca\")\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
